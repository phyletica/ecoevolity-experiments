% Dispersal cartoon
% Rendering issues
% Use "excluded" constant characters to avoid confusion with "missing"
% Emphasize that dpp-msbayes is biased toward shared events in discussion

\section{Introduction}

To understand the distribution of Earth's biodiversity, we must consider the
degree to which environmental changes explain diversity within and among
species.
A major component of this is understanding how community-scale processes cause
co-diversification across evolutionary lineages.
Such processes are expected to generate patterns of divergence times that are
difficult to explain by lineage-specific processes of diversification.
Specifically, finding that divergences are temporally clustered across
multiple evolutionary lineages provides compelling evidence that a shared
process was responsible for the lineages diverging.
For example, the fragmentation of an environment, like an island, forest, or
watershed, can cause multiple taxa distributed across that environment to
co-diverge over a short period relative to evolutionary timescales
(Figure~\ref{fig:divCartoon}).
% \thought{Better word than ``environment?'' I don't like landscape, because
%     it excludes aquatic systems.}
% , such that a model that treats them as
% simultaneous will be a better explanation of the genetic variation than a model
% that treats the divergence of each lineage as independent \figureNeeded.
One way to test the predictions of such processes of diversification is to
infer the temporal pattern of divergences across multiple taxa, and determine
whether any subsets of the taxa shared the same divergence times.
% As a result, we stand to learn a lot about processes of diversification from
% a robust method for inferring the temporal pattern of divergences across
% multiple, independent lineages.

\begin{linenomath}
If researchers are interested in comparing the divergence times among a number
of pairs of populations,
we can approach this as a problem of model choice:
How many divergence events, and what assignment of taxa to those events, best
explain the genetic variation within and between the diverged populations of
each pair (Figure~\ref{fig:divCartoon})?
One challenge of this inference problem is the number of possible models.
If we have \ncomparisons{} pairs of populations, we would like to assign them to an
unknown number of divergence events, \nevents{}, which can range from one to
\ncomparisons{}.
For a given number of divergence events, the Stirling number of the second kind
tells us the number of ways of assigning the taxa to the divergence times
(i.e., the number of models with \nevents{} divergence-time parameters):
\begin{equation}
    S_2(\ncomparisons, \nevents) = 
    \frac{1}{\nevents!} \sum_{i = 0}^{\nevents - 1} (-1)^{i}
    \binom{\nevents}{i} (\nevents - i)^{\ncomparisons}.
    \label{eq:stirling2}
\end{equation}
When the number of divergence times is unknown, we need to sum over all
possible values of \nevents{} to get the total number of possible divergence
models
\citep[the Bell number;][]{Bell1934}):
\begin{equation}
    B_{\ncomparisons} = \sum_{\nevents = 1}^{\ncomparisons}
    S_2(\ncomparisons, \nevents).
    \label{eq:bell}
\end{equation}
As the number of pairs we wish to compare grows, the prospect of comparing
maximum or marginal likelihoods among all possible models quickly becomes
daunting.
As a result, a Bayesian model-averaging approach is appealing, because it
allows the data to determine which models are most relevant.
\end{linenomath}

Methods have been developed to perform this model averaging using
approximate-likelihood Bayesian computation (ABC)
\citep{Hickerson2006,Huang2011,Oaks2014dpp}.
However, these methods often struggle to detect multiple divergence times
across pairs of populations \citep{Oaks2012, Oaks2014reply} or have little
information to update \emph{a priori} expectations \citep{Oaks2014dpp}.
More fundamentally, the loss of information inherent to ABC approaches can
prevent them from discriminating among models
\citep{Robert2011,Marin2014,Green2015}.

One proposed solution is to focus the inference problem on whether or not all
pairs diverged at the same time (i.e., $\nevents = 1$ versus
$\nevents > 1$) \citep{Hickerson2013}.
However, limiting the inference in this way is often not satisfactory, because
biogeographers rarely expect that all of the pairs of populations they wish
to compare diverged at the same time.
Limiting ourselves to the hypothesis of a single shared divergence would not
recognize situations where only a subset of taxa co-diverged, or where multiple
shared divergences have occurred.
The latter is particularly relevant when multiple landscape changes are known
to have occurred.
More fundamentally,
\citet{Papadopoulou2016}
astutely point out that all of the pairs co-diverging is not the correct null
hypothesis.
If we wish to test for shared divergences, it is more appropriate to consider
all the pairs diverging independently as the null expectation.

Here, our goal is to develop a new Bayesian model-choice approach to this
problem that
handles many more genetic loci,
takes full advantage of the information in those loci,
and therefore more reliably
estimates the number of divergence events and the assignment of taxa to those
events.
% We introduce a new Bayesian model-choice method for inferring
% shared divergence times across taxa.
Our method leverages recent analytical work \citep{Bryant2012} to efficiently
and directly compute the full-likelihood of divergence models from genomic
data.
By efficiently using all of the information in the data, the new method is
faster,
more accurate,
and more precise
than approximate-likelihood methods for
estimating shared divergences.
We introduce the new method and its assumptions, assess its performance with
simulated data, and apply it to genomic data from geckos from the Philippine
Islands.


\section{Methods}

\subsection{The data}
We assume we have genetic data from multiple pairs of populations, and our goal
is to estimate the time at which the two populations of each pair diverged, and
compare these divergence times across the pairs.
For each pair of populations that we wish to compare, we assume that we have
collected orthologous genetic markers with at most two states.
We will refer to these as ``biallelic characters,'' but note that this includes
constant characters (i.e., characters for which all the samples
from the two populations share the same state).
We follow \citet{Bryant2012} in referring to the two possible
states as ``red'' and ``green.''
We assume each character is effectively unlinked, i.e., each marker evolved
along a gene tree that is independent of the others, conditional on the
population history.
Examples include well-spaced, single-nucleotide polymorphisms (SNPs) or
amplified fragment-length polymorphisms (AFLPs).

For each population and for each marker we sample \allelecount
copies of the locus, \redallelecount of which are copies of the red
allele and the remaining $\allelecount - \redallelecount$ are
copies of the green allele;
\redallelecount can range from zero to \allelecount.
Thus, for each population of a pair, and for each locus, we have a count of the
total sampled gene copies and how many of those are the red allele.

We will use \leafallelecounts and \leafredallelecounts to denote allele counts
for a locus from both populations of a pair; i.e., 
$\leafallelecounts, \leafredallelecounts = (\allelecount[1], \redallelecount[1]), 
(\allelecount[2], \redallelecount[2])$
(Figure~\ref{fig:divCartoon}).
When we will also use ``character pattern'' to refer to $\leafallelecounts,
\leafredallelecounts$.
We will use \comparisondata[i] to denote these counts across all the loci from
population pair $i$.
In other words, \comparisondata[i] is all the genetic data collected from
population pair $i$.
Finally, we use \alldata to represent the data across all the pairs of
populations of which we wish to compare the divergence times.
Note, because the pairs are unconnected
(Figures~\labelcref{fig:divCartoon,fig:dag}),
different characters can be collected for each pair (i.e., characters do not
need to be orthologous across the pairs).


\subsection{The model}

\subsubsection{The evolution of markers}

% We begin unpacking our model by first focusing on a single pair of populations.
We assume a finite-sites, continuous-time Markov chain (CTMC) model for the
evolution of the biallelic characters along a gene tree with branch lengths,
\genetree.
As the marker evolves along the gene tree, forward in time, there is an
instantaneous relative rate \rgmurate of mutating from the red state to the
green state, and a corresponding relative rate \grmurate of mutation from green
to red.
The stationary frequency of the red and green state is then
$\grmurate / (\rgmurate + \grmurate)$
and
$\rgmurate / (\rgmurate + \grmurate)$, respectively.
Thus, if given the stationary frequency of the green allele, \gfreq, we can
obtain the relative rates of mutation between the two states.
We will denote the overall rate of mutation as \murate.
If a mutation rate per site per unit time is given, then branch lengths are in
absolute time.
Alternatively, if $\murate = 1$, the branch lengths of the gene tree are in
units of expected substitutions per site.
In such a case, for a given pair of populations, the \murate is 
redundant, because it can be incorporated into the branch lengths of the gene
tree.
However, we introduce the notation here, because it will be useful later when
we want to allow rate variation among pairs of populations.
% For now, we will assume $\murate = 1$ so that the gene tree branch lengths, and
% time in general, is in units of the expected substitutions per site.

\subsubsection{The evolution of gene trees}

We assume that each marker sampled from a pair of populations evolved within a
simple ``species'' tree with one ancestral root population that diverged into
two descendant (terminal) branches at time \comparisondivtime
(Figures~\labelcref{fig:divCartoon,fig:dag}).
Again, if the $\murate$ is given, \comparisondivtime is in units of absolute
time; however, if $\murate$ is set to one, time is in units of expected
substitutions per site.
We will use
\comparisonpopsizes{}
to denote all three
effective sizes of a population pair
(\epopsize[\rootpopindex],
\epopsize[\descendantpopindex{1}],
and \epopsize[\descendantpopindex{2}]).
We will also use
\sptree{}
as shorthand for the species tree, which comprises the population sizes and
divergence time of a pair
(\comparisonpopsizes{} and \comparisondivtime{}).

\subsubsection{The likelihood}

\begin{linenomath}
% Given \murate, \gfreq, \comparisondivtime and \comparisonpopsizes,
Given \murate, \gfreq, and \sptree{},
the probability of the observed data at a locus (\leafallelecounts and
\leafredallelecounts), is the probability of the character pattern given the
gene tree multiplied by the probability of the gene tree given the species
tree, summed over all possible gene tree topologies and integrated over all
possible gene tree branch lengths,
\begin{equation}
    \pr(\leafallelecounts, \leafredallelecounts \given \sptree, \murate, \gfreq)
    =
    \int_{\genetree}
    \pr(\leafallelecounts, \leafredallelecounts \given \genetree, \murate, \gfreq)
    \pr(\genetree, \murate, \gfreq \given \sptree)
    \diff{\genetree}
    \label{eq:markerlikelihood}
\end{equation}
\citep{Felsenstein1988,Nielsen2001,Rannala2003}.
% We take advantage of the mathematical work of \citep{Bryant2012} to
% analytically integrate over all possible gene trees and character
% mutational histories along those gene trees.
We take advantage of the mathematical work of \citep{Bryant2012} to
analytically integrate over all possible gene trees and all possible character
substitution histories along those gene trees.
This allows us to compute the likelihood of the species tree directly from a
biallelic character pattern under a coalescent model.
% \thought{For completeness, should I lay out the math and algorithm for
%     calculating the likelihood in the supplemental materials?}
\end{linenomath}

\begin{linenomath}
Assuming independence among loci (conditional on the species tree), we can
calculate the probability of \nloci{} loci given the species tree by
simply taking the product over them,
\begin{equation}
    \pr(\comparisondata \given \sptree, \murate, \gfreq)
    =
    \prod_{i=1}^{\nloci}
    \pr(\leafallelecounts[i], \leafredallelecounts[i] \given \sptree, \murate, \gfreq).
    \label{eq:comparisonlikelihood}
\end{equation}
Finally, the likelihood across all of our \ncomparisons{} pairs is simply the
product of the likelihood of each pair,
\begin{equation}
    \pr(
    \alldata
    \given
    \sptrees,
    \murates,
    \gfreqs)
    =
    \prod_{i=1}^{\ncomparisons}
    \pr(\comparisondata[i] \given \sptree[i], \murate[i], \gfreq[i]),
    \label{eq:collectionlikelihood}
\end{equation}
where
$\alldata = \comparisondata[1], \comparisondata[2], \ldots, \comparisondata[\ncomparisons]$,
$\sptrees = \sptree[1], \sptree[2], \ldots, \sptree[\ncomparisons]$,
$\murates = \murate[1], \murate[2], \ldots, \murate[\ncomparisons]$,
and
$\gfreqs = \gfreq[1], \gfreq[2], \ldots, \gfreq[\ncomparisons]$.
\end{linenomath}

\subsubsection{Correcting for excluded constant characters}

\begin{linenomath}
If we exclude constant characters and only analyze variable characters, we need
to correct the sample space for the excluded constant characters.
We can correct the likelihood by simply dividing by the probability of a
variable character, which is equal to one minus the probability of a constant
character,
\begin{equation}
\begin{split}
    \pr(\leafallelecounts, \leafredallelecounts \given \sptree, \murate, \gfreq, \textrm{variable})
    & =
    \frac{
        \pr(\leafallelecounts, \leafredallelecounts \given \sptree, \murate, \gfreq)
    }{
        \pr(\textrm{variable} \given \sptree, \murate, \gfreq)
    } \\
    & =
    \frac{
        \pr(\leafallelecounts, \leafredallelecounts \given \sptree, \murate, \gfreq)
    }{
        1 - \pr(\textrm{constant} \given \sptree, \murate, \gfreq)
    } \\
    & =
    \frac{
        \pr(\leafallelecounts, \leafredallelecounts \given \sptree, \murate, \gfreq)
    }{
        1 - \pr(\leafallelecounts \textrm{ all red} \given \sptree, \murate, \gfreq)
        - \pr(\leafallelecounts \textrm{ all green} \given \sptree, \murate, \gfreq)
    }.
    \label{eq:variablemarkerlikelihood}
\end{split}
\end{equation}
When we take the product over loci to get the probability of all the variable
data collected from a pair of populations, we correct each character pattern to
allow for different numbers of sampled gene copies among loci,
\begin{equation}
    \pr(\comparisondata \given \sptree, \murate, \gfreq, \textrm{variable})
    =
    \prod_{i=1}^{\nloci}
    \frac{
        \pr(\leafallelecounts[i], \leafredallelecounts[i] \given \sptree, \murate, \gfreq)
    }{
        1 - \pr(\leafallelecounts[i] \textrm{ all red} \given \sptree, \murate, \gfreq)
        - \pr(\leafallelecounts[i] \textrm{ all green} \given \sptree, \murate, \gfreq)
    }.
    \label{eq:variablecomparisonlikelihood}
\end{equation}
This is a bit different than the correction done in the software SNAPP
\citep{Bryant2012}.
If we use \maxleafallelecounts to denote the maximum number of gene copies
sampled from each population, then the correction in SNAPP is
\begin{equation}
    \pr_{\tiny SNAPP}(\comparisondata \given \sptree, \murate, \gfreq, \textrm{variable})
    =
    \frac{
        \prod_{i=1}^{\nloci}
        \pr(\leafallelecounts[i], \leafredallelecounts[i] \given \sptree, \murate, \gfreq).
    }{
        (1 - \pr(\maxleafallelecounts \textrm{ all red} \given \sptree, \murate, \gfreq)
        - \pr(\maxleafallelecounts \textrm{ all green} \given \sptree, \murate, \gfreq))^{\nloci}
    }.
    \label{eq:snappvariablecomparisonlikelihood}
\end{equation}
These are equivalent if the same number of samples are collected across all
variable loci for each population (i.e., no missing gene copies), but will
deviate if fewer copies are sampled for at least one locus.
Thus, identical likelihoods between SNAPP and our method should not be expected
when analyzing variable-only data.
\end{linenomath}

\subsection{Bayesian inference}

\begin{linenomath}
We can obtain a posterior probability distribution by naively plugging the
likelihood in Equation~\ref{eq:collectionlikelihood} into Bayes' rule,
\begin{equation}
    \pr(
    \sptrees,
    \murates,
    \gfreqs
    \given
    \alldata
    )
    =
    \frac{
        \pr(
        \alldata
        \given
        \sptrees, \murates, \gfreqs
        )
        \pr(
        \sptrees,
        \murates,
        \gfreqs
        )
    }{
        \pr(
        \alldata
        )
    }.
    \label{eq:collectionindependentbayesrule}
\end{equation}
% By expanding out the species trees into their component parts, the divergence
% times and effective population sizes, we get
% \begin{equation}
%     \pr(
%     \comparisondivtimes,
%     \collectionpopsizes,
%     \murates,
%     \gfreqs
%     \given
%     \alldata
%     )
%     =
%     \frac{
%         \pr(
%         \alldata
%         \given
%         \comparisondivtimes,
%         \collectionpopsizes,
%         \murates,
%         \gfreqs
%         )
%         \pr(\comparisondivtimes, \collectionpopsizes, \murates, \gfreqs)
%     }{
%         \pr(
%         \alldata
%         )
%     }.
%     \label{eq:collectionindependentbayesruleexpanded}
% \end{equation}
However, this assumes all pairs of populations diverged independently, not
allowing us to learn about shared divergence times.
% this would be the same as calculating the posterior of each pair
% separately.
What we want to do is relax this assumption and allow pairs to share divergence
times.
% We want to estimate how many divergences occurred, and which pairs, if any,
% shared divergences.
\end{linenomath}

Let's use \divtimemodel to represent the divergence model, which comprises the
divergence times---the number of which (\nevents{}) can range from 1 to
\ncomparisons{}---and the mapping of non-overlapping subsets of the population
pairs to these \nevents{} divergence times.
We will separate out \divtimemodel into two components,
\begin{enumerate}
    \item the partitioning of the \ncomparisons{} population pairs to
        divergence events, which we will denote as \divtimesets, and
    \item the divergence times themselves,
        $\divtimes = \divtime[1], \ldots, \divtime[\nevents]$.
\end{enumerate}
We relax the assumption of independent divergence times by treating the number
of divergence events and the assignment of population pairs to those events as
random variables under a Dirichlet process \citep{Ferguson1973,
    Antoniak1974}.
Specifically, we use the Dirichlet process as a prior on divergence models,
$\divtimemodel \sim \dirp(\basedistribution, \concentration)$, where
\basedistribution is the base distribution of the process and \concentration is
concentration parameter that controls how clustered the process is.
The concentration parameter determines the prior probability of
\divtimesets
(the partitioning of the population pairs)
and the base distribution determines the prior probability of the divergence
time of each subset.

\begin{linenomath}
Under the Dirichlet process prior, the posterior becomes
\begin{equation}
    \pr(
    \concentration,
    \divtimemodel,
    \collectionpopsizes,
    \murates,
    \gfreqs
    \given
    \alldata,
    \basedistribution
    )
    =
    \frac{
        \pr(
        \alldata
        \given
        \divtimemodel,
        \collectionpopsizes,
        \murates,
        \gfreqs
        )
        \pr(\divtimemodel \given \concentration, \basedistribution)
        \pr(\concentration)
        \pr(\collectionpopsizes)
        \pr(\murates)
        \pr(\gfreqs)
    }{
        \pr(
        \alldata,
        \basedistribution
        )
    },
    \label{eq:bayesrule}
\end{equation}
where
\collectionpopsizes
is the collection of the effective population sizes (\comparisonpopsizes{})
across pairs.
By expanding the divergence model (\divtimemodel) into the unique
divergence times (\divtimes) and the partitioning of the pairs of populations
to those times (\divtimesets), we get
\begin{equation}
    \pr(
    \concentration,
    \divtimes,
    \divtimesets,
    \collectionpopsizes,
    \murates,
    \gfreqs
    \given
    \alldata,
    \basedistribution
    )
    =
    \frac{
        \pr(
        \alldata
        \given
        \divtimes,
        \divtimesets,
        \collectionpopsizes,
        \murates,
        \gfreqs
        )
        \pr(\divtimesets \given \concentration)
        \pr(\divtimes \given \divtimesets, \basedistribution)
        \pr(\concentration)
        \pr(\collectionpopsizes)
        \pr(\murates)
        \pr(\gfreqs)
    }{
        \pr(
        \alldata,
        \basedistribution
        )
    }.
    \label{eq:bayesruleexpanded}
\end{equation}
\end{linenomath}

\subsubsection{Priors}

% \paragraph{Prior on divergence models}
% As mentioned above, we treat the number of divergence events and the assignment
% of population pairs to those events as random variables under a Dirichlet
% process prior \citep{Ferguson1973, Antoniak1974}.

\begin{linenomath}
\paragraph{Prior on the concentration parameter}
Given a single parameter, \concentration, the Dirichlet process determines the
prior probability of all the possible ways the \ncomparisons{} pairs of
populations can be partitioned to $\nevents = 1, 2, \ldots \ncomparisons$
divergence events.
Given \concentration, the prior probability that two pairs of populations, $i$
and $j$ (assuming $i \neq j$), share the same divergence time is
\begin{equation}
    \pr(\comparisondivtime[i] = \comparisondivtime[j] \given \concentration)
    =
    \frac{1}{1 + \concentration}
\end{equation}
This illustrates that when \concentration is small, the process tends to be
more clumped, and as it increases, the process tends to favor more independent
divergence times.
One option is to simply fix the concentration parameter to a particular value,
which is likely sufficient when the number of pairs is small.
Alternatively, we allow a hierarchical approach to accommodate uncertainty in
the concentration parameter by specifying a gamma distribution as a prior on
\concentration \citep{Escobar1995,Heath2011}.
\end{linenomath}

\paragraph{Prior on the divergence times}
Given the subsets of pairs assigned to divergence events, we use a gamma
distribution for the prior on the divergence time of each event,
$\divtime \given \divtimesets \sim \distgamma(\cdot, \cdot)$.
This is the base distribution (\basedistribution) of the Dirichlet process.

\paragraph{Prior on the effective population sizes}
For the two descendant populations of each pair, we use a gamma distribution as
the prior on the effective population sizes.
For the root population, we use a gamma distribution on the effective
population size \emph{relative} to the mean size of the two descendant
populations, which we denote as \rootrelativepopsize.
For example, a value of one would mean the root population size is equal to 
$(\epopsize[\descendantpopindex{1}] + \epopsize[\descendantpopindex{2}]) / 2$.
The goal of this approach is to allow more informative priors on the root
population size; we often have stronger prior expectations for the relative
size of the ancestral population than the absolute size.
This is important, because the effective size of the ancestral population is a
difficult nuisance parameter to estimate and can be strongly correlated with
the divergence time.
For example, if the divergence time is so old such that all the gene copies
of a locus coalesce within the descendant populations, the locus
provides very little information about the size of the ancestral
population.
As a result, a larger ancestral population and more recent divergence will have
a very similar likelihood to a small ancestral population and an older
divergence.
Thus, placing more prior density on reasonable values of the ancestral
population size can help improve the precision of divergence-time estimates.

\paragraph{Prior on mutation rates}
In the model presented above, for each population pair, the divergence time
(\divtime) and mutation rate (\murate) are inextricably linked.
For a single pair of populations, if little is known about the mutation rate,
this problem is easily solved by setting it to one ($\murate[1] = 1$) such
that time is in units of expected substitutions per site and the effective
population sizes are scaled by \murate.
However, what about the second pair of populations for which we wish to compare
the divergence time to the first?
Because the species trees in our model are disconnected
(Figures~\labelcref{fig:divCartoon,fig:dag}),
we cannot learn about
the relative rates of mutation across the population pairs from the data.
As a result, we need strong prior information about the relative rates of
mutation across population pairs for this model to work.

If the second pair of populations is closely related to the first, and shares a
similar life history, we could assume they share the same mutation rate and
set the mutation rate of the second pair to one as well ($\murate[1] = \murate[2] = 1$).
Alternatively, we could relax that assumption and put a prior on \murate[2].
However, this should be a strongly informative prior.
Placing a weakly informative prior on \murate[2] would mean that we can no
longer estimate its divergence time relative to the first pair.
So, while it is possible to incorporate uncertainty in relative mutation rates,
it is important to keep in mind that the data cannot inform these parameters,
and thus the uncertainty will be directly reflected in the uncertainty of
divergence times.

\paragraph{Prior on the equilibrium-state frequency}
Our method allows for a beta prior to be placed on the frequency of the green
allele for each pair of populations,
$\gfreq[i] \sim \textrm{Beta}(\cdot, \cdot)$.
However, if using SNP data, we advise fixing the frequency of the red and green
states to be equal (i.e., $\gfreq = 0.5$).
The reason for this is that there is no natural way of re-coding four-state
nucleotides to two states, and so the relative transition rates, \rgmurate and
\grmurate, are not biologically meaningful.
There will always be arbitrariness associated with how one decides to perform
this re-coding, and unless $\gfreq = 0.5$, this arbitrariness will affect the
likelihood and results.
Constraining \gfreq to 0.5 makes the CTMC model a two-state analog of the
``JC69'' model \citep{JC1969}.
However, if the genetic markers are naturally biallelic, the frequencies of the
two states can be meaningfully estimated, making the model a two-state general
time-reversible model \citep{Tavare1986}.

\input{mcmc.tex}

\subsection{Software implementation}
The method outlined above is implemented in the open-source software package,
\ecoevolity, written in the \cpp language.
The source code is freely available from
\url{https://github.com/phyletica/ecoevolity}, and documentation is available
at
\url{http://phyletica.org/ecoevolity/}.
The software package is accompanied by an extensive test suite, which, among
other aspects,
validates that the likelihood code returns the same values
as
SNAPP \citep{Bryant2012},
and all of our MCMC proposals sample from the expected prior distribution
when data are ignored.

The \ecoevolity package includes four programs:
\begin{enumerate}
    \item \ecoevolity for performing Bayesian inference under the
        model described above.
    \item \sumcoevolity for summarizing posterior samples collected by
        \ecoevolity and performing simulations to calculate Bayes factors for
        all possible numbers of divergence events.
    \item \simcoevolity for simulating biallelic characters under the model
        described above.
    \item \dpprobs for Monte Carlo approximations of probabilities under the
        Dirichlet process; this can be useful for choosing a prior on the
        concentration parameter.
\end{enumerate}
We have also developed a Python package, \pycoevolity, to help with
preprocessing data and summarizing posterior samples collected by \ecoevolity.
This includes assessing MCMC chain stationarity and convergence and plotting
posterior distributions.
The source code for \pycoevolity is available at
\url{https://github.com/phyletica/pycoevolity}.

All of our analyses were performed with
Version 0.1 
(commit 1d688a3)
of the \ecoevolity software package.
The \timerootsizemixer algorithm implemented in this version of the software
only updates one ancestral population size per proposal.
In Version 0.2 (commit 884780e), the default behavior is for the
\timerootsizemixer proposal to update the ancestral population size for all
other pairs associated with the same divergence time (see above).
While this tends to improve mixing slightly, it does not change the results we
present here in a meaningful way.
Our results can be reproduced exactly with Version 0.1.


\subsection{Analyses of simulated data}

\subsubsection{Validation analyses}
Our first step to validate the new method was to verify that it behaves as
expected when the model is correct (i.e., data are simulated and analyzed under
the same model).
We used the \simcoevolity tool from the \ecoevolity package, which simulates
data under the model described above.
All data were simulated under the following settings:
\begin{enumerate}
    \item $\ncomparisons = 3$
    \item $\concentration = 1.414216$, which corresponds with a prior mean of
        $\nevents = 2$ divergence events
    \item $\divtime \sim \dexponential{0.01}$
    \item $\gfreq = 0.5$
    \item $\murate = 1$
\end{enumerate}
We simulated data under five different settings for the effective population
sizes.
The first setting was an idealized situation where all population sizes were
known and equal,
$\epopsize[\rootpopindex] = 
\epopsize[\descendantpopindex{1}] =
\epopsize[\descendantpopindex{2}] = 0.002$.
The four remaining scenarios differed in their distribution on the relative
effective size of the root population:
\begin{enumerate}
    \item $\rootrelativepopsize \sim \dgamma{2}{1}$
    \item $\rootrelativepopsize \sim \dgamma{10}{1}$
    \item $\rootrelativepopsize \sim \dgamma{100}{1}$
    \item $\rootrelativepopsize \sim \dgamma{1000}{1}$
\end{enumerate}
For these four scenarios, the descendant populations were distributed as
\dgamma{5}{0.002}.
The most difficult nuisance parameter to estimate for a pair of populations is
the root population size, which can be correlated with the parameter of
interest, the divergence time.
Thus, our choice of simulation settings is designed to assess how uncertainty
in the root population size affects inference.

Under each of the five scenarios we simulated 500 \datasets of 100,000
characters and 500 \datasets of 500,000 characters.
This includes constant characters; the mean number of variable SNPs was
approximately 5,500 and 27,500, respectively.
We then analyzed all 5,000 simulated \datasets in \ecoevolity both with and
without constant characters included.
For all analyses, the prior for each parameter matched the distribution
the true value was drawn from when the data were simulated.
For analyses where 
$\epopsize[\rootpopindex] = 
\epopsize[\descendantpopindex{1}] =
\epopsize[\descendantpopindex{2}] = 0.002$,
we ran
three independent MCMC chains for 37,500 generations, sampling every
25th generation.
For all other analyses, we ran the three chains for 75,000 generations,
sampling every 50th generation.
As a result, we collected 4503 samples for each analysis (1501 samples from
each chain, including the initial state).

In order to assess the frequentist behavior of the posterior probabilities of
divergence models inferred by \ecoevolity, we simulated an additional
20,000 \datasets of 100,000 characters under the setting where
$\rootrelativepopsize \sim \dgamma{100}{1}$.
All 20,500 \datasets were analyzed with \ecoevolity and binned based on the
inferred posterior probability that $\nevents{} = 1$.
The mean posterior probability that $\nevents{} = 1$ for each bin was plotted
against the proportion of \datasets within the bin for which the true
divergence model was $\nevents{} = 1$;
the latter approximates the true probability that $\nevents{} = 1$.
If the new method is unbiased, in a frequentist sense, the inferred posterior
probabilities that $\nevents{} = 1$ within a bin should approximately equal the
proportion of the \datasets for which that is true
\citep{Huelsenbeck2004,Oaks2012,Oaks2014dpp}.

\subsubsection{Assessing the effect of linked characters}
The characters of most \datasets being collected by high-throughput technologies
do not all evolve along independent gene trees.
Most consist of many putatively unlinked loci that each comprise sequences of
linked nucleotides.
For example, ``RADseq'' and ``sequence capture'' techniques generate thousands
of loci that are approximately 50--300 nucleotides in length.
This creates a question when using methods like \ecoevolity that assume each
character is independent:
Is it better to violate the assumption of unlinked characters and use all of
the data, or throw away much of the data to avoid linked characters?

To better adhere to the unlinked-character assumption, we could retain only a
single site per locus.
However, this results in a very large loss of data.
Furthermore, to try and maximize the informativeness of the retained
characters, most researchers retain only one \emph{variable} character per
locus.
While this can be corrected for (see
Equation~\ref{eq:variablecomparisonlikelihood}), it still results in the loss
of a very informative component of the data: The proportion of variable
characters.
Before throwing away so much information, we should determine whether
it is in our best interest.
In other words, does keeping all of the data and violating the assumption of
unlinked characters result in better or worse inferences than throwing out much
of our data?

To address this question, 
we simulated \datasets composed of loci of linked sites that were 100, 500, and
1000 characters long.
The characters for each locus were simulated along the same gene tree (i.e., no
intra-locus recombination).
Simulated \datasets were analyzed with \ecoevolity in one of three ways:
(1) All characters were included,
(2) only variable characters were included,
and
(3) only a maximum of one variable character per locus was included.
Only the last option avoids violating the assumption of unlinked characters,
but throws out the most data.

For all three locus lengths, we simulated 500 \datasets with a total of 100,000
and 500,000 characters.
The settings of the simulations performed with \simcoevolity, and subsequent
analyses with \ecoevolity, correspond with the validation analyses described
above where the relative size of the root population was distributed as
\dgamma{100}{1}.
Furthermore, to assess the affect of linked characters on the posterior
probabilities of divergence models, we simulated an additional 10,000 \datasets
with 1,000, 100-character loci (100,000 total characters each).
As described above, to assess the frequentist behavior of the inferred
posterior probabilities, we binned the results of the analyses of these 10,500
\datasets based on the posterior probability that $\nevents{} = 1$ and plotted
the mean of each bin against the approximated true probability that $\nevents{}
= 1$.

\subsubsection{Assessing the effect of missing data}
The method should be robust to missing data, because it is simply treated as a
smaller sample of gene copies from a particular population for a particular
locus.
Because each character is assumed to have evolved along a coalescent
gene tree, the identity of each gene copy within a population does not matter.
Thus, some loci having fewer sampled gene copies from some populations should
result in more variance in parameter estimates, but is not expected to create
bias.
To confirm this behavior, we simulated \datasets with different probabilities of
sampling each gene copy.
Specifically, we simulated \datasets for which the probability of sampling each
gene copy was 90\%, 75\%, or 50\%, which resulted in \datasets with
approximately 10\%, 25\%, or 50\% missing data.
For each sampling probability, we simulated 100 \datasets with 500,000
characters; the settings were the same as described for the validation analyses
above where $\rootrelativepopsize \sim \dgamma{100}{1}$.

\subsubsection{Assessing the effect of biases in character-pattern acquisition}
When analyzing the \spp{Gekko} data (see below), we observed large
discrepancies in the estimated divergence times depending on whether or not the
constant characters were removed from the analysis.
This was not observed in the analyses of simulated data, because the likelihood
is appropriately corrected for the excluded constant characters.
This suggests that there are additional character-pattern acquisition biases in
the empirical data, for which are our method cannot correct.
Such acquisition biases have been documented during the denovo assembly of
RADseq loci \citep{Harvey2015,Linck2017}.

The loss of rare alleles during the acquisition and assembly of the data could
explain the much larger divergence times estimated from the empirical data when
constant characters are removed.
After the constant characters, the rare alleles are ``next in line'' to inform
the model that the population divergence was recent.
If these patterns are being lost during data acquisition and assembly, and not
accounted for in the likelihood calculation, this should create an upward bias
in the divergence time estimates.

To explore whether data acquisition bias can explain the discrepancy we
observed for the \spp{Gekko} data, we simulated \datasets where the probability
of sampling singleton character patterns (i.e., one gene copy is different from
all the others) was 80\%, 60\%, and 40\%.
For each, we simulated and analyzed 100 \datasets; the settings were
the same as described for the validation analyses above where
$\rootrelativepopsize \sim \dgamma{100}{1}$.


\subsubsection{Comparison to ABC methods}
We wanted to compare the performance of the new method to the existing
approximate-likelihood Bayesian computation (ABC) method \dppmsbayes 
\citep{Oaks2014dpp}.
In order to do this, we had to simulate relatively small \datasets that the ABC
methods could handle in a reasonable amount of time.
Accordingly, we simulated \datasets with 200 loci, each with 200 linked
characters (40,000 total characters).
For analyses of both \ecoevolity and \dppmsbayes, the settings were
\begin{enumerate}
    \item $\ncomparisons = 3$
    \item $\concentration = 1.414216$, which corresponds with a prior mean of
        $\nevents = 2$ divergence events
    \item $\divtime \sim \dgamma{2}{0.05}$
    \item $\murate = 1$
    \item $\epopsize[\descendantpopindex{}] \sim \dgamma{5}{0.002}$
\end{enumerate}
For analyses with \dppmsbayes, the effective size of the ancestral population,
\epopsize[\rootpopindex], was also distributed as \dgamma{5}{0.002}, whereas
for \ecoevolity, the relative effective size of the ancestral population,
\rootrelativepopsize, was distributed as \dgamma{100}{1};
the marginal prior on \epopsize[\rootpopindex] induced by the latter is similar
to the former.
For analyses with \dppmsbayes, we assumed a Jukes-Cantor model of nucleotide
substitution, whereas for the \ecoevolity, we assumed the two-state equivalent
(i.e., $\gfreq = 0.5$).

Each method was applied to 500 \datasets simulated under its own model.
Thus, there were no model violations, except for the new method, for which the
assumption of unlinked characters was violated by the 200-character loci.
For the analysis of each simulated \dataset with \ecoevolity, three independent
MCMC chains were run for 75,000 generations, sampling every 50th generation.
For the \dppmsbayes analyses, 500,000 samples were simulated from the joint
prior distribution, 2,000 of which were retained for the approximate posterior
sample.

\subsection{Empirical application}
Previous methods for estimating shared divergence times often over-cluster taxa
\citep{Oaks2012,Oaks2014reply} or have little information to update prior
expectations \citep{Oaks2014dpp}.
Thus, a good empirical test of the new method would be pairs of populations
that we expect diverged independently of one another.
We analyzed RADseq data from four pairs of populations of \spp{Gekko} lizards;
the collection of these data are detailed in \citep{Oaks2018paic}.
Each pair of populations inhabit two different oceanic islands in the
Philippines that were never connected during lower sea levels of glacial
periods.
Because these islands were never connected, the divergence between the
populations of each pair is likely due to over-water dispersal, the timing of
which should be idiosyncratic to each pair.

We analyzed the data with and without the constant characters.
Also, there were a small number of sites that had more than two nucleotides
represented, which cannot be handled directly by our model of biallelic
characters.
We explored two ways of handling these sites:
(1) excluding them, and
(2) coding the first nucleotide in the alignment as 0 (``green''), and all
other nucleotides for that site as 1 (``red'').
Thus, between including/excluding the constant sites and removing/re-coding the
polyallelic characters, we analyzed four versions of the RADseq data.

To be conservative in assessing the ability of the new method to distinguish
divergence times among the pairs, we set $\concentration = 0.44$, which places
50\% of the prior probability on one divergence event (i.e., all four pairs
sharing the same divergence).
Furthermore, to assess the sensitivity of the results to the \concentration, we
also tried $\concentration = 3.77$, which corresponds with a prior mean number
of divergence events of three.
Other settings that were shared by all analyses of the \spp{Gekko} RADseq data
include:
\begin{itemize}
    \item $\epopsize[\descendantpopindex{}] \sim \dgamma{4}{0.004}$
    \item $\rootrelativepopsize \sim \dgamma{100}{1}$
    \item $\gfreq = 0.5$
    \item $\murate = 1$ for all four pairs
\end{itemize}

The ABC methods of inferring shared divergence events are very sensitive to the
prior on divergence times
\citep{Oaks2012,Hickerson2013,Oaks2014reply,Oaks2014dpp}.
To assess whether results of our new method are also sensitivity to the
prior on divergence times, we analyzed the \datasets that included constant
characters under the following priors:
\begin{enumerate}
    \item $\divtime \sim \dexponential{0.005}$
    \item $\divtime \sim \dexponential{0.01}$
    \item $\divtime \sim \dexponential{0.05}$
    \item $\divtime \sim \dexponential{0.1}$
    \item $\divtime \sim \dexponential{0.2}$
\end{enumerate}
For the two versions of the \spp{Gekko} data that lacked the constant
characters, we used the following priors:
\begin{enumerate}
    \item $\divtime \sim \dexponential{0.01}$
    \item $\divtime \sim \dexponential{0.05}$
    \item $\divtime \sim \dexponential{0.1}$
    \item $\divtime \sim \dexponential{0.2}$
    \item $\divtime \sim \dexponential{0.5}$
\end{enumerate}

For all analyses, we ran 10 independent MCMC chains for 150,000 generations,
sampling every 100th generation.
Convergence and mixing of the chains was assessed by the potential scale
reduction factor (the square root of Equation 1.1 in
\citet{Brooks1998}) and effective sample size \citep{Gong2014} of the
likelihood and all parameters.
We also inspected the chains visually with the program Tracer version 1.6
\citep{Tracer16}.


\section{Results}

\subsection{Analyses of simulated data}

\subsubsection{Validation analyses}

When there is no model misspecification, our new method has the desired
frequentist behavior wherein 95\% of the time the true value of a parameter
falls within the 95\% credibility interval.
We see this for
divergence times
(Figure~\ref{fig:valdivtimes})
and the effective sizes of
descendant
(Figure~S\ref{fig:valleafsizes})
and ancestral
(Figure~S\ref{fig:valrootsizes})
populations.
Our results also show that the estimated posterior probability of the single
divergence model ($\nevents{} = 1$) mirrors the probability that the model
is correct 
(Figure~\ref{fig:valpostprobs}).
We see the same behaviors whether or not the constant characters are excluded,
demonstrating that our likelihood correction for excluded constant
characters is working correctly
(Equation~\ref{eq:variablecomparisonlikelihood}).

As expected, the precision of divergence time and population size estimates is
greater when the constant characters are included and when there is greater
prior information about the ancestral population size
(Figures \ref{fig:valdivtimes}, S\ref{fig:valleafsizes}, \&
S\ref{fig:valrootsizes}).
The increase in precision associated with the fivefold increase in the number
of sampled characters (100k to 500k) is relatively modest (Figures
\ref{fig:valdivtimes}, S\ref{fig:valleafsizes}, \& S\ref{fig:valrootsizes}).
% suggesting that the benefit of collecting more characters begins to plateau
% when \datasets are small relative to the number of characters commonly
% collected via modern high-throughput sequencing technologies (i.e., the
% simulated 100k \datasets had only 5,500 SNPs on average).
Retaining the constant characters results in a much larger increase in
precision than collecting five times more characters.

The true number of divergence events is included in the 95\% credibility set
greater than 98\% of the time for all the simulation conditions
(Figure~\ref{fig:valnevents}).
The frequency at which the correct number of events has the largest posterior
probability, and the median posterior probability of the correct number of
events,
increases when constant characters are retained and as prior information about
the ancestral population size increases 
(Figure~\ref{fig:valnevents}).
As with the parameter estimates, the performance increase associated with the
increase from 100k to 500k sampled characters is moderate; retaining the
constant characters has a much larger effect (Figure~\ref{fig:valnevents}).
When constant characters are used, the median posterior
probability of the correct number of divergence event is high (over 0.89
for all simulation conditions; 
Figure~\ref{fig:valnevents}).

For the \datasets simulated with 100,000 and 500,000 characters, the number of
variable characters ranged from
515--21,676 
and
4,670--105,373,
respectively, with an average of approximately
5,500
and 
27,500 variable characters, respectively
(Figures S\ref{fig:valnumberofvariablesites100k} \&
S\ref{fig:valnumberofvariablesites500k}).
As expected, the variance in the number variable characters increases with the
variance in the prior distribution of the relative effective size of the root
population
(Figures S\ref{fig:valnumberofvariablesites100k} \&
S\ref{fig:valnumberofvariablesites500k}).

The MCMC chains for all analyses converged very quickly;
we conservatively removed the first 401 samples, resulting in 3300 samples from
the posterior (1100 samples from three chains) for each analysis.
To assess convergence and mixing, we plotted histograms of the potential scale
reduction factor across the three independent chains and the effective sample
size for the log-likelihood and divergence times
(Figures S\ref{fig:valpsrflikelihood}, S\ref{fig:valpsrfdivtimes},
S\ref{fig:valesslikelihood}, \& S\ref{fig:valessdivtimes}).
Mixing was poorer when there was more prior uncertainty in the root population
size
(Figures S\ref{fig:valesslikelihood} \& S\ref{fig:valessdivtimes}).
However, given the expected frequentist behavior for how often the true
parameter values were contained within the 95\% confidence intervals (Figures
\ref{fig:valdivtimes}, S\ref{fig:valleafsizes}, \& S\ref{fig:valrootsizes}),
and the weak relationship between the ESS and estimation error
(Figure~S\ref{fig:valessvserror}),
we do not expect MCMC mixing had a large effect on our simulation results under
the most extreme levels of uncertainty in the root population size that we
simulated.



\subsubsection{Assessing the effect of linked characters}

The accuracy of divergence time estimates did not appear to be affected by the
model violation of linked characters
(Figures \ref{fig:linkagedivtimes500k}
\&
S\ref{fig:linkagedivtimes100k}).
However,
as the length of loci increases, we do see an underestimation
of posterior uncertainty (i.e., the 
true divergence time is contained within the 95\% credibility interval 
less frequently than 95\% of the time;
Figures \ref{fig:linkagedivtimes500k}
\&
S\ref{fig:linkagedivtimes100k}).
This makes sense given that there is less coalescent variation in the data than
the model expects if all the characters had evolved along independent gene
trees.
Importantly, this effect of underestimating posterior uncertainty is small
for \datasets with 100bp loci, suggesting this violation of the model has little
impact for high-throughput \datasets with short loci, like those collected via
RADseq.
As expected, analyzing only one variable site per locus removes this underestimation
of posterior uncertainty
(see the last row of Figures \ref{fig:linkagedivtimes500k}
\&
S\ref{fig:linkagedivtimes100k}),
but at a large cost of much greater posterior uncertainty in parameter
estimates due to the loss of data.
We see the same behavior for estimating the effective sizes of the ancestral
and descendant populations
(Figures S\labelcref{fig:linkageleafsizes100k,fig:linkageleafsizes500k,fig:linkagerootsizes100k,fig:linkagerootsizes500k}).

The cost of removing data to avoid violating the assumption of unlinked
characters is also very pronounced for estimating the number of divergence
events.
The method better estimates the correct number of events, and with much higher
posterior probability, when the constant characters are retained 
(Figures
\ref{fig:linkagenevents500k}
\&
S\ref{fig:linkagenevents100k}).
The median posterior probability of the correct number of divergence events is
over 0.95 for all 500k-character \datasets, even when loci were 1000bp long
(Figure~\ref{fig:linkagenevents500k}).
However, our results show that linked characters do introduce bias in the
estimated posterior probability of the one divergence model ($\nevents{} = 1$)
(Figure~\ref{fig:linkagepostprobs}).
However, the bias is moderate and makes the method conservative in the
sense that it tends to underestimate the probability of shared
divergence (Figure~\ref{fig:linkagepostprobs}).

For simulated \datasets with loci of length 100, 500, and 1000 base pairs, there
were an average of 5.4, 27.1, and 54.1 variable characters per locus,
respectively.
As expected, the number of variable characters per 100k and 500k \dataset was
very similar to the simulated unlinked-character \datasets, with an average of
about
5,500 variable characters per 100k \dataset
(Figure~S\ref{fig:linkagenumberofvariablesites100k})
and
27,100 variable characters per 500k \dataset
(Figure~S\ref{fig:linkagenumberofvariablesites500k}).
When at most one variable character is sampled per locus, the
number of remaining characters is usually close or equal to the
number of loci;
1000, 200, and 100 characters for the 100k \datasets
with 100, 500, and 1000 bp loci, respectively, and
5000, 1000, and 500 characters for 500k \datasets
with 100, 500, and 1000 bp loci, respectively
(Figures
S\ref{fig:linkagenumberofvariablesites100k}
\&
S\ref{fig:linkagenumberofvariablesites500k}).


\subsubsection{Assessing the effect of missing data}

As predicted by coalescent theory, our results show that random missing data
has little effect on the performance of the method with respect to estimating
divergence times
(Figure~\ref{fig:missingdivtimes}),
effective population sizes
(Figures
S\ref{fig:missingleafsizes}
\&
S\ref{fig:missingrootsizes}), or the number of divergence events
(Figure~\ref{fig:missingnevents}).


\subsubsection{Assessing the effect of biases in character-pattern acquisition}

Biased character acquisition against singleton character patterns does create
bias in estimates of divergence times
(Figure~\ref{fig:filtereddivtimes}) and
population sizes
(Figure
S\ref{fig:filteredleafsizes}
\&
S\ref{fig:filteredrootsizes}), and the bias increases as the probability of
missing a character with a singleton pattern increases.
Notably, this bias is smaller when the constant characters are retained in
the \dataset
(Figure
\ref{fig:filtereddivtimes},
S\ref{fig:filteredleafsizes}
\&
S\ref{fig:filteredrootsizes}).

However, in the face of data-acquisition bias, the method still estimates the
number of divergence events well, especially when constant characters are used
(Figure~\ref{fig:filterednevents}).
Even when the probability of sampling a character with a singleton pattern is
0.4, the median posterior probability of the correct number of divergence
events is 0.948.
(Figure~\ref{fig:filterednevents}).


\subsubsection{Comparison to ABC methods}

The new full-likelihood method, \ecoevolity, does a much better job of
estimating divergence times
(Figure~\ref{fig:bakeoffdivtimes})
and effective population sizes
(Figures
S\ref{fig:bakeoffleafsizes}
\&
S\ref{fig:bakeoffrootsizes}),
than the approximate-likelihood Bayesian method, \dppmsbayes.
This is despite the simulated \datasets being ``tailored'' for the ABC method
(i.e., loci of 200 linked base pairs).
Notably, the new method does not underestimate the older divergence times like
the ABC method, which suffers from saturated population-genetic summary
statistics that assume an infinite-sites model of mutation
(Figure~\ref{fig:bakeoffdivtimes}).

The new method also does a better job of estimating the number of divergence
events (Figure~\ref{fig:bakeoffnevents}), with a median posterior probability
of the correct number of events of 0.942, compared to 0.789 for the ABC method.
Importantly, the new method underestimates the number of events much less
frequently (Figure~\ref{fig:bakeoffnevents}),
which should lead to fewer erroneous interpretations of shared processes of
divergence.

It is difficult to compare the computational effort between the two approaches,
given that \ecoevolity is collecting autocorrelated samples from
the full posterior, whereas \dppmsbayes is collecting independent samples
from a different distribution we hope is similar to the posterior.
To compare the overall amount of computation required by the two approaches
we look at the average time it takes to analyze a simulated \dataset
on a single processor 
(2.6GHz Intel Xeon CPU E5-2660 v3).
This was 34.6 days for \dppmsbayes (2,991,304 seconds)
and only 33.4 minutes (2004.5 seconds) for \ecoevolity.
The majority of the runtime for the ABC method is spent simulating samples
from the prior distribution.
While this step can be parallelized, the likelihood computations of \ecoevolity
can also be multi-threaded.
Regardless of the difficulties associated with comparing the approaches, the
1,492-fold difference in computation time clearly demonstrates the
full-likelihood method is much more efficient than ABC.


\subsection{Empirical application}

When the new method is applied to all of the RADseq sites from the
four pairs of \spp{Gekko} populations, the results strongly
support that all of the pairs diverged independently
(Figure~\ref{fig:gekkonevents}).
The results are very robust to the priors on divergence times (\divtime)
and the concentration parameter (\concentration) of the Dirichlet process, and
to whether the polyallelic SNPs are recoded as binary
(Figure~\ref{fig:gekkonevents})
or removed
(Figure~S\ref{fig:gekkonopolynevents}).
Likewise, the estimates of divergence times and effective population sizes
are nearly identical regardless of the prior on \divtime or \concentration,
or whether polyallelic SNPs are recoded or removed
(Figures
\ref{fig:gekkodivtimes},
S\ref{fig:gekkosizes},
S\ref{fig:gekkonopolydivtimes},
\&
S\ref{fig:gekkonopolysizes}).

However, when only variable SNPs are analyzed, the behavior is much different.
First, the estimated divergence times and population sizes are clearly far too
large and more sensitive to the priors on the divergence times and the
concentration parameter
(Figures
S\ref{fig:gekkovaronlydivtimes}
\&
S\ref{fig:gekkovaronlysizes}).
While the true values of these parameters are obviously unknown, given the
variability of these data (Table~\ref{table:gekkocomparisons}), and other data
from these species \citep{Siler2012, Siler2014kikuchii}, these values are
clearly nonsensical.
Also, the posterior probabilities of the number of divergences is also
much more sensitive to the \divtime and \concentration priors,
with some combinations yielding results for which three divergence events
are preferred, although Bayes factors always preferred four divergences
(Figure~S\ref{fig:gekkovaronlynevents}).
These findings are similar when the polyallelic SNPs are removed
(Figures
S\labelcref{fig:gekkonopolyvaronlydivtimes,fig:gekkonopolyvaronlysizes,fig:gekkonopolyvaronlynevents}).

The large overestimation of divergence times and population sizes is consistent
with our findings from the \datasets simulated with an acquisition bias against
rare allele patterns
(Figure
\ref{fig:filtereddivtimes},
S\ref{fig:filteredleafsizes}
\&
S\ref{fig:filteredrootsizes}).
In these simulation-based analyses, we also saw dramatic overestimation of
these parameters when constant characters were excluded.
It appears that variable character patterns are being lost during the acquisition and
assembly of the RADseq data, and the model is sensitive to these missing
variable sites, especially when only variable characters are analyzed.


\section{Discussion}

Previous approaches to estimating shared divergence times based on
approximate-likelihood Bayesian computation (ABC) are very sensitive to prior
assumptions about divergence times and often over-cluster divergences with
strong support
\citep{Oaks2012,Hickerson2013,Oaks2014reply,Oaks2014dpp}.
Here, we introduced a new approach that increases the power and robustness of
these inferences by leveraging all of the information in genomic data within a
full-likelihood, Bayesian framework.
The full-likelihood approach is much better at estimating
divergence times
(Figure~\ref{fig:bakeoffdivtimes})
and nuisance parameters
(Figure S\ref{fig:bakeoffleafsizes} \& S\ref{fig:bakeoffrootsizes})
than ABC.
It is also better able to estimate the correct number of divergence events with
more confidence, and is much less biased toward underestimating the number of
divergence events
(Figure~\ref{fig:bakeoffnevents}).
This is especially important, because most biogeographers that use these
methods are interested in testing for shared events.
The increased power of the method to detect variation in divergence times and
avoid spurious estimates of shared divergences will lead to fewer erroneous
interpretations of shared processes of divergence.

The efficiency associated with using all of the information in the data makes
the method very promising for empirical applications.
For example, increasing the number of characters from 100,000 to 500,000
resulted in only modest improvement in precision
(Figures
\ref{fig:valdivtimes}, S\ref{fig:valleafsizes}, \& S\ref{fig:valrootsizes}).
This suggests that the benefit of collecting more characters begins to plateau
when \datasets are small relative to the number of characters commonly
collected via modern high-throughput sequencing technologies (i.e., the
simulated 100k \datasets had only 5,500 SNPs on average).
Even with only 200 short (200 bp) loci, the median posterior probability
of the correct number of divergence events was 0.94
(Figure~\ref{fig:bakeoffnevents}).
Also, directly calculating the likelihood of the population history from
genomic data avoids the computation necessary for approximating the likelihood
via simulations.
As a result, the new method, \ecoevolity, could provide better approximations
of the posterior over 1000 times faster than the ABC method, \dppmsbayes.


\subsection{To exclude linked characters, or not?}

The increased precision and robustness associated with retaining constant
characters creates an interesting question when analyzing DNA sequence data
from reduced-representation genomic libraries:
Is it better to analyze all the data and violate the assumption that the
characters are unlinked, or suffer a large loss of data to avoid violating that
assumption?
Several of our results suggest retaining all the data is preferable.
First of all, the method is much better at estimating the divergence times,
effective population sizes, and the correct number of events with high
posterior probability when analyzing linked sequences of characters compared to
when only one variable character per locus is analyzed
(Figures
\labelcref{fig:linkagedivtimes500k,fig:linkagenevents500k},
and
Figures
S\labelcref{fig:linkagedivtimes100k,fig:linkageleafsizes500k,fig:linkageleafsizes100k,fig:linkagerootsizes500k,fig:linkagerootsizes100k,fig:linkagenevents100k}).
Second, retaining all the data makes the method more robust to data-acquisition
biases
(Figures
\labelcref{fig:filtereddivtimes,fig:filterednevents}
and
Figures
S\labelcref{fig:filteredleafsizes,fig:filteredrootsizes}),
which are common in alignments from reduced-representation genomic
libraries
\citep{Harvey2015,Linck2017}.
Third, the results from the \emph{Gekko} RADseq data are
reasonable and robust to prior assumptions when all data are analyzed,
but nonsensical and sensitive to prior assumptions when only variable
characters are analyzed.
Our simulations suggest this is due to the filtering of the character patterns
that occurred when assembling these data.

Perhaps most striking is how much better the method estimates the number
of divergence events when all the data are used.
For example, across the 500,000-character \datasets, the
median posterior probability of the correct number of divergence events
is over 0.94 regardless of the linked characters
or pattern-acquisition biases we simulated
(Figures
\ref{fig:valnevents},
\ref{fig:linkagenevents500k},
\&
\ref{fig:filterednevents}).
For comparison,
these values are as low as 0.41 when constant characters are removed
\ref{fig:filterednevents}).

We caution against generalizing our findings
of favorable performance with linked loci
to other methods that assume unlinked characters.
However, 
\citet{ChifmanKubatko2014}
found quartet inference of splits in multi-species coalescent trees
from SNP data was also robust to the violation of unlinked characters.
Our results show the amount of data that is discarded to avoid linked
characters can far outweigh the effects of violating the assumption of
unlinked characters.
When analyzing linked loci with a method that assumes unlinked characters,
using simulations to assess the effect of linkage on the method may be worth
the effort in order to bring more data to bear.


\subsection{Philippine \spp{Gekko}}
We purposefully selected a challenging empirical test case for the new method.
Each of the four pairs of populations of \spp{Gekko} occur on two different
oceanic islands that were never connected.
Thus, we do not expect shared divergence times across the pairs.
However, based on previous findings \citep{Siler2012, Siler2014kikuchii}, all
these pairs likely diverged very recently.
This is a challenging region of parameter space for this type of method: Very
similar and recent divergence times that are nonetheless independent.
% We used the new method to compare the divergence times of four pairs
% of populations of \spp{Gekko} inhabiting eight islands in the Philippines.
% Each pair of populations inhabit two different oceanic islands that were never
% connected, and thus never experienced the fragmentation associated with
% Pleistocene sea-level fluctuations.
Our results strongly support independent divergences, despite all four pairs
diverging very recently.
(Figures
\ref{fig:gekkonevents}
\& 
\ref{fig:gekkodivtimes}).
These results demonstrate that using the likelihood from genomic data provides
enough information to unambiguously separate divergences across very narrow
timescales.

\subsection{Caveats}
This method is subject to the caveats associated with all model-based
statistical methods,
however, there are two caveats that are worth emphasizing with
specific reference to the types of models we explored here.
First, it is important to keep in mind that when modeling the divergence of two
populations, the time of the divergence and the mutation rate are inextricably
linked.
Thus, we cannot learn about the relative rates of mutation among pairs of
populations when also trying to estimate their divergence times.
Unlike previous methods \citep{Hickerson2006,Huang2011,Oaks2014dpp},
we allow priors to be placed on mutation rates, to allow uncertainty
to be incorporated into the model.
However, the priors on the mutation rates need to be informative if one hopes
to be able to estimate the divergence times.

Second, the new method does not allow migration after populations diverge.
This is a weakness compared to ABC approaches to this problem
\citep{Huang2011,Oaks2014dpp}.
However, given the bias and sensitivity to priors exhibited by the ABC
methods even when migration is ignored
\citep{Oaks2012,Oaks2014reply,Oaks2014dpp},
modeling migration with these methods is not advisable without thorough
simulation-based analyses to assess their statistical behavior.

\section{Conclusions}
We introduced a new Bayesian model-choice method for estimating shared
divergence times across taxa.
By using the full likelihood and genome-scale data, the new method is much more
accurate, precise, and robust than existing methods based on approximate
likelihoods.
This new tool will allow biologists to leverage comparative genomic data to
test hypotheses about the effects of environmental change on diversification.
